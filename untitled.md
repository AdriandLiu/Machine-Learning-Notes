---
description: ML Notes
---

# Machine Learning ALL

ML Midterm Review

Contents

[Overview 4](broken-reference)

[Logistics 5](broken-reference)

[ML Basics 5](broken-reference)

[Measures 5](broken-reference)

[Terminology 5](broken-reference)

[Nearest Neighbor Classifier 5](broken-reference)

[K - nearest neighbors 5](broken-reference)

[Curse of Dimensionality 6](broken-reference)

[Manifold Hypothesis 6](broken-reference)

[Model Selection 6](broken-reference)

[Generalization 6](broken-reference)

[Hyperparameter Tuning 6](broken-reference)

[K-folder cross validation CV 6](broken-reference)

[Leave-one-out CV 6](broken-reference)

[Inductive (归纳的) Bias 6](broken-reference)

[Linear 7](broken-reference)

[Representing Data 7](broken-reference)

[Linear Model 7](broken-reference)

[Loss Function 7](broken-reference)

[Logistics 8](broken-reference)

[Solution 8](broken-reference)

[Logistic Regression Model: 8](broken-reference)

[Loss functions for linear classifier 8](broken-reference)

[Find the optimal weights 8](broken-reference)

[Gradient 8](broken-reference)

[Likelihood 9](broken-reference)

[Maximum likelihood & logistic regression 9](broken-reference)

[Maximum likelihood & linear regression 9](broken-reference)

[Multiclass classification 9](broken-reference)

[Softmax likelihood & one-hot encoding 9](broken-reference)

[Optimization 9](broken-reference)

[Gradient descent! 9](broken-reference)

[Naïve Bayes 10](broken-reference)

[Naïve bayes model 10](broken-reference)

[Max joint likelihood 10](broken-reference)

[Naïve bayes formula details: 10](broken-reference)

[Class prior 10](broken-reference)

[Likelihood 11](broken-reference)

[![](.gitbook/assets/0.png) 11](broken-reference)

[Binary features: Bernoulli NB 11](broken-reference)

[Multinomial features: 11](broken-reference)

[Gaussian features: 11](broken-reference)

[Decision Boundary 11](broken-reference)

[Discriminative vs Generative Classification 11](broken-reference)

[Regularization 11](broken-reference)

[Ridge regression 11](broken-reference)

[Ridge weight formula 12](broken-reference)

[Ridge with data normalization 12](broken-reference)

[Maximum a Posteriori (MAP) 12](broken-reference)

[Gaussian prior 12](broken-reference)

[Laplace prior 12](broken-reference)

[Lasso 12](broken-reference)

[L1 vs L2 regularization 12](broken-reference)

[Diff regularization subset selection 13](broken-reference)

[L1 VS L0 13](broken-reference)

[Bias-variance decomposition 13](broken-reference)

[Cross validation 13](broken-reference)

[Evaluation 14](broken-reference)

[Gradient Descent 14](broken-reference)

[Optimization in ML 14](broken-reference)

[Gradient 14](broken-reference)

[Gradient descent 14](broken-reference)

[Convex function 14](broken-reference)

[Definition 14](broken-reference)

[Why convex? 14](broken-reference)

[Recognizing convex functions 15](broken-reference)

[Gradient for linear and logistic 15](broken-reference)

[![](.gitbook/assets/1.png) 15](broken-reference)

[Time complexity 15](broken-reference)

[Codes 15](broken-reference)

[Learning rate alpha 15](broken-reference)

[Stochastic gradient descent 15](broken-reference)

[Convergence of SGD 16](broken-reference)

[Minibatch SGD 16](broken-reference)

[Codes 16](broken-reference)

[Momentum 16](broken-reference)

[Codes 17](broken-reference)

[Adagrad (Adaptive gradient) 17](broken-reference)

[RMSprop (Root Mean Squared Propagation) 17](broken-reference)

[Adam (Adaptive Moment Estimation) 17](broken-reference)

[Adding L2 Regularization 17](broken-reference)

[Adding L1 Regularization 18](broken-reference)

[Evaluation 18](broken-reference)

[Evaluation and comparison 18](broken-reference)

[Performance metrics for classification 18](broken-reference)

[Example precision recall 18](broken-reference)

[Less common metrics: 19](broken-reference)

[Performance metrics for multi-class classiﬁcation 19](broken-reference)

[Trade-oﬀ between precision and recall ROC\&AUC 19](broken-reference)

[Cross validation in Evaluation 19](broken-reference)

[Over-ﬁtting in Model Selection 19](broken-reference)

[Nested CV 19](broken-reference)

[Perceptron and Support Vector Machines 20](broken-reference)

[Perceptron 20](broken-reference)

[Objective 20](broken-reference)

[Optimization 20](broken-reference)

[Codes 20](broken-reference)

[Issues 20](broken-reference)

[Margin 20](broken-reference)

[Max margin classifier 21](broken-reference)

[Hard margin SVM objective 21](broken-reference)

[Soft margin SVM constraints 21](broken-reference)

[Hinge loss 21](broken-reference)

[Perceptron vs SVM 21](broken-reference)

[![](.gitbook/assets/2.png) 22](broken-reference)

[SVM codes 22](broken-reference)

[SVM recap 22](broken-reference)

[SVM vs. logistic regression 22](broken-reference)

[Decision Trees 23](broken-reference)

[Pros and cons 23](broken-reference)

[DT idea 23](broken-reference)

[Prediction per region 23](broken-reference)

[For regression: 23](broken-reference)

[For classification: 23](broken-reference)

[Feature types 23](broken-reference)

[Continuous: 23](broken-reference)

[Ordinal: 23](broken-reference)

[Categorical: 24](broken-reference)

[Cost Function 24](broken-reference)

[Regression cost 24](broken-reference)

[Classification cost 24](broken-reference)

[Search space 24](broken-reference)

[Greedy heuristic 24](broken-reference)

[Stopping the recursion 24](broken-reference)

[Entropy loss 25](broken-reference)

[Why? 25](broken-reference)

[Mutual information 25](broken-reference)

[Entropy for classiﬁcation cost 25](broken-reference)

[Gini index – the way to split the tree 26](broken-reference)

[Overfitting 26](broken-reference)

[Pruning 26](broken-reference)

[Bootstrap, bagging, and boosting 26](broken-reference)

[Bootstrap 26](broken-reference)

[Codes 26](broken-reference)

[Bagging (Bootstrap aggregating) 27](broken-reference)

[Bagging for regression 27](broken-reference)

[Bagging for classiﬁcation 27](broken-reference)

[Example 27](broken-reference)

[Random forests 27](broken-reference)

[Out of bag (OOB) samples 27](broken-reference)

[Summary 28](broken-reference)

[Adaptive bases 28](broken-reference)

[Optimization idea 28](broken-reference)

[Example 28](broken-reference)

[Exponential loss 29](broken-reference)

[AdaBoost algorithm 29](broken-reference)

[Discrete AdaBoost Algorithm Example 30](broken-reference)

[Example 30](broken-reference)

[Gradient boosting 30](broken-reference)

[Algorithm 30](broken-reference)

[Gradient tree boosting 31](broken-reference)

[Some loss functions for gradient boosting 31](broken-reference)

[!!!Boosting vs Bagging 31](broken-reference)

[Multilayer perceptron 33](broken-reference)

[Adaptive Radial Base 33](broken-reference)

[Sigmoid Bases 34](broken-reference)

[Adaptive sigmoid bases 34](broken-reference)

[Multilayer perceptron MLP 34](broken-reference)

[Regression 34](broken-reference)

[Classification 35](broken-reference)

[!!! Activation function 35](broken-reference)

[Logistic function 35](broken-reference)

[Hyperbolic tangent 35](broken-reference)

[ReLU 35](broken-reference)

[Leaky ReLU 35](broken-reference)

[Softplus 35](broken-reference)

[Network architecture 35](broken-reference)

[Depth vs Width 35](broken-reference)

[Multilayer perceptron 36](broken-reference)

[Regularization strategies 37](broken-reference)

[Overfit: variance reduction 37](broken-reference)

[Noise robustness 37](broken-reference)

[Early stopping 37](broken-reference)

[Bagging 37](broken-reference)

[Dropout 38](broken-reference)

[Summary 38](broken-reference)

[Gradient Computation & Automatic Differentiation 38](broken-reference)

[Landscape of the cost function 39](broken-reference)

[Jacobian matrix 39](broken-reference)

[Chain rule 39](broken-reference)

[Training a two layer MLP 39](broken-reference)

[Gradient calculation 39](broken-reference)

[For regression 39](broken-reference)

[For binary classification 39](broken-reference)

[For multiclass classification 40](broken-reference)

[Example: 40](broken-reference)

[Automating gradient computation 41](broken-reference)

[Automatic differentiation 41](broken-reference)

[Forward mode 41](broken-reference)

[Computational graph 41](broken-reference)

[Reverse mode 41](broken-reference)

[Computational graph 42](broken-reference)

[Forward vs Reverse mode 42](broken-reference)

[Summary 42](broken-reference)

[Backpropagation explain and resources 42](broken-reference)

Overview

Types:

Supervised learning: labeled data:

**Classification** (categorical output, discrete):

**Regression** (numerical output, continuous),

**Structured prediction**

Unsupervised learning: only unlabeled data

Clustering

Dimensionality reduction

Density estimation / generative modeling

Anomaly detection

Discovering latent factors and structures

## Logistics

ML Basics

## Measures

![](.gitbook/assets/3.png) Binary Classification

## Terminology

![](.gitbook/assets/4.png)

![](.gitbook/assets/5.png)

![](.gitbook/assets/6.png)

![](.gitbook/assets/7.png)

## Nearest Neighbor Classifier

**training**: do nothing

**test**: predict the lable by ﬁnding the closest image in the training set and need a measure of _**distance**_

![](.gitbook/assets/8.png)

## K - nearest neighbors

**training**: do nothing (lazy learning)

**test**: predict the lable by ﬁnding the K closest instances

![](.gitbook/assets/9.png)

![](.gitbook/assets/10.png)

a non-parametric method: the number of model parameters **grows** with the data this is in contrast with a **parametric model** which has a **ﬁxed** number of parameters, is faster to use but has stronger assumptions on the nature of the data distribution&#x20;

**KNN works well if input has low dimensions**

## Curse of Dimensionality

High dim are unintuitive

* need exponentially more instances for K-NN to work (more data)
* with same number of instances, the space becomes very sparse
* **all samples have similar distances**

## Manifold Hypothesis

In the real-world, high-dim data lie close to the surface of a manifold (多支管)/low-dim

![](.gitbook/assets/11.png)

## Model Selection

Example: KNN – k is the hyper-parameter, k up classification boundary smoother, training error increase; too small: overfit, too large: underfit

## Generalization

Generalization: performance of algorithm on unseen data

![](.gitbook/assets/12.png)

## Hyperparameter Tuning

Training (used to train the model),

Testing (used to evaluate the final model),

Validation dataset (used to tune the hyperparameter).

_Performance on validation set ≈ generalizationError_

### K-folder cross validation CV

* ![](.gitbook/assets/13.png)partition the data into k folds
* use k-1 for training, and 1 for validation
* average the validation error over all folds

### Leave-one-out CV

Extreme case of k = N

## Inductive (归纳的) Bias

ML algorithms need to make assumptions about the problem

inductive bias: strength and correctness of assumptions are important in having good performance

Examples:

* manifold hypothesis in KNN (and many other methods)
* close to linear dependencies in linear regression
* conditional independence and causal structure in probabilistic graphical models

Linear

## Representing Data

![](.gitbook/assets/14.png)

![](.gitbook/assets/15.png)

we assume N instances in the dataset each instance has D features indexed by d

## Linear Model

![](.gitbook/assets/16.png)

![](.gitbook/assets/17.png)

## Loss Function

Objective: find parameters to fit the data

![](.gitbook/assets/18.png)

Residual: ![](.gitbook/assets/19.png)

Linear least square (L2 loss) cost function:

![](.gitbook/assets/20.png)

**L1 vs L2 Loss Function**

L1 and L2 are two loss functions in machine learning which are used to minimize the error.

L1 Loss function stands for **Least Absolute Deviations**. Also known as LAD.

![](.gitbook/assets/21.png)

L2 Loss function stands for **Least Square Errors**. Also known as LS.

![](.gitbook/assets/22.png)

**AIMS TO MIN LOSS FUNTION:**

![](.gitbook/assets/23.png)

Derivative:

![](.gitbook/assets/24.png) ![](.gitbook/assets/25.png)

Direct Solution:

![](.gitbook/assets/26.png)

![](.gitbook/assets/27.png)

Linear for **large dataset:**

Stochastic gradient descent

what if X^T X is **not invertible**? (columns of X are not linearly independent, either redundant features or numFeature > numInstance; D> N)

![](.gitbook/assets/28.png)

Logistics

**Logistics is a linear classifier**

more than one class:  y ∈ {0,1,…,C} (**multi-class**: logistics fit C number of classifier and make prediction based on the most confident result), fit a linear model to each class, turn y into one-hot encoding: ![](.gitbook/assets/29.png)

Linear regression is sensitive to the **outliers**

L2 loss **problem**: correct prediction can have higher loss than the incorrect one! ![](.gitbook/assets/30.png)

Solution: Squash the loss function: ![](.gitbook/assets/31.png)

Logistics:

![](.gitbook/assets/32.png)

Decision boundary:

![](.gitbook/assets/33.png)

## Logistic Regression Model:

![](.gitbook/assets/34.png)

## Loss functions for linear classifier

![](.gitbook/assets/35.png)

Simplifying the cost function

![](.gitbook/assets/36.png)

## Find the optimal weights

### Gradient

![](.gitbook/assets/37.png)

Logit function: ![](.gitbook/assets/38.png)

## Likelihood

![](.gitbook/assets/39.png)

![](.gitbook/assets/40.png)

### Maximum likelihood & logistic regression

Cross Entropy loss deviated from log-likelihood of Bernoulli PDF

Minimizing logistic loss corresponds to maximizing **Bernoulli** likelihood. Minimizing squared-error loss corresponds to maximizing **Gaussian** likelihood (it's just OLS regression; for 2-class classification it's actually equivalent to LDA).

Log likelihood: why? Likelihood value blows up for large N, work with log-likelihood instead (same maximum) cross entropy

![](.gitbook/assets/41.png)

### Maximum likelihood & linear regression

Squared error loss:

![](.gitbook/assets/42.png)

## Multiclass classification

**Binary** classification: **Bernoulli** likelihood

![](.gitbook/assets/43.png)

C classes: categorical likelihood

**Softmax**

Softmax做事情就是对**最大值**进行强化 (for the reason why exp(z\_i))

![](.gitbook/assets/44.png) ![](.gitbook/assets/45.png)

![](.gitbook/assets/46.png)

if input values are large, **softmax** becomes similar to **argmax**

![](.gitbook/assets/47.png)

### Softmax likelihood & one-hot encoding

**Why one-hot encoding**: The integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship

![](.gitbook/assets/48.png)

we can also use this encoding for **categorical inputs features**  one-hot encoding for input features

![](.gitbook/assets/49.png)

**Problem:** these features are not linearly independent, why? might become an issue for linear regression. why

**Solution:**

remove one of the one-hot encoding features

![](.gitbook/assets/50.png)

## Optimization

### Gradient descent!

![](.gitbook/assets/51.png)

![](.gitbook/assets/52.png)

Naïve Bayes

**Discriminative**: conditional distribution p(y|x) eg: linear and logistics

**Generative**: joint distribution p(x,y) = p(y)p(x|y) eg:

&#x20;Naïve bayes ![](.gitbook/assets/53.png)

Example:

![](.gitbook/assets/54.png)

**in a generative classiﬁer likelihood & prior class probabilities are learned from data**

Some generative classiﬁers:

* Gaussian Discriminant Analysis: the likelihood is multivariate Gaussian
* Naive Bayes: decomposed likelihood

## Naïve bayes model

NB **assumption**: ![](.gitbook/assets/55.png)

when features are **conditionally independent** given the label

## Max joint likelihood

Joint distribution: p(x,y) = p(y)p(x|y)

![](.gitbook/assets/56.png)

During training of NB:

![](.gitbook/assets/57.png)

During testing of NB: ![](.gitbook/assets/58.png)

![](.gitbook/assets/59.png)

## Naïve bayes formula details:

## Class prior

**Choice of class prior depends on the type of classes**

![](.gitbook/assets/60.png)

**Binary classification:**

Bernoulli distribution: ![](.gitbook/assets/61.png)

Max log-likelihood & get the max point: ![](.gitbook/assets/62.png)

**Multiclass classification:**

Categorical distribution: ![](.gitbook/assets/63.png)

Optimal parameters: ![](.gitbook/assets/64.png)

## Likelihood

**Choice of likelihood distribution depends on the type of features**

## ![](.gitbook/assets/65.png)

* Bernoulli: binary features
* Categorical: categorical features
* Gaussian: continuous distribution

**each feature may use a diﬀerent likelihood**

**and separate max-likelihood estimate for each feature** ![](.gitbook/assets/66.png)

## Binary features: Bernoulli NB

MLE: ![](.gitbook/assets/67.png)

Implementation:

![](.gitbook/assets/68.png)

## Multinomial features:

Multinomial likelihood: ![](.gitbook/assets/69.png)

MLE estimates: ![](.gitbook/assets/70.png)

## Gaussian features:

![](.gitbook/assets/71.png)

MLE![](.gitbook/assets/72.png)

## Decision Boundary

two classes have the same probability: ![](.gitbook/assets/73.png)

![](.gitbook/assets/74.png)

### Discriminative vs Generative Classification

![](.gitbook/assets/75.png)

Regularization

**Avoid overfitting**

**When overfitting, we often see large weights**

Idea: penalize large parameter values

## Ridge regression

**L2 regularized** linear least squares regression:

![](.gitbook/assets/76.png)

Side note L2 norm:

![](.gitbook/assets/77.png)

* regularization parameter  λ > 0 controls the strength of regularization
* a good practice is to not penalize the intercept

## Ridge weight formula

![](.gitbook/assets/78.png)

## Ridge with data normalization

**Without regularization**: ![](.gitbook/assets/79.png)

![](.gitbook/assets/80.png)

**With regularization**: ![](.gitbook/assets/81.png)

Diff features will be penalized differently

![](.gitbook/assets/82.png)

**Instead of maximize log-likelihood, we maximize the posterior**

![](.gitbook/assets/83.png)

## Maximum a Posteriori (MAP)

![](.gitbook/assets/84.png)

## Gaussian prior

![](.gitbook/assets/85.png)

## Laplace prior

Lasso ![](.gitbook/assets/86.png)

## L1 vs L2 regularization

![](.gitbook/assets/87.png)

![](.gitbook/assets/88.png)

## Diff regularization subset selection

![](.gitbook/assets/89.png)

optimizing this is a diﬃcult combinatorial problem:

* search over all    2^D     subsets

## L1 VS L0

It’s just like LASSO but has a little difference. LASSO has a limit:

the L1 norm of the parameters < t (some constant threshold)

For L0 regularization. The constraint is the number of parameters < t (some constant threshold)

Most people never heard about it because LASSO is good enough in the cases that people want to punish some parameters to zero. L0 regularization shares the same function with it. **The difference is L0 is more extreme than L1. The parameters are much easier to be punished to zero.**

If you have 500 features in the pool and you want 10 of them left, you can try LASSO. However, if you have 10k features in the pool and you want 10 of them left, you probably want to try L0 regularization.

## Bias-variance decomposition

![](.gitbook/assets/90.png)

![](.gitbook/assets/91.png)

![](.gitbook/assets/92.png)

![](.gitbook/assets/93.png)

Larger regularization penalty -> high bias – low variance

high variance in more complex models means that test and training error can be very diﬀerent

high bias in simplistic models means that training error can be high

![](.gitbook/assets/94.png)

## Cross validation

k-fold CV

![](.gitbook/assets/95.png)

leave-one-out CV:extreme case of k=N

Test data:

once the hyper-parameters are selected, we can use the whole set for training use test set for the **ﬁnal** assessment

## Evaluation

![](.gitbook/assets/96.png)

ROC receiver operating characteristic

![](.gitbook/assets/97.png)

How to graph: [https://acutecaretesting.org/en/articles/roc-curves-what-are-they-and-how-are-they-used](https://acutecaretesting.org/en/articles/roc-curves-what-are-they-and-how-are-they-used)

Gradient Descent

## Optimization in ML

![](.gitbook/assets/98.png)

## Gradient

Partial derivate ![](.gitbook/assets/99.png)

![](.gitbook/assets/100.png)

## Gradient descent

Iterative algorithm for optimization

![](.gitbook/assets/101.png)

## Convex function

### Definition

![](.gitbook/assets/102.png)

Any two points can be connected by at most one line

![](.gitbook/assets/103.png)

### Why convex?

Convex is easier to minimize:

* Critical pts are the global minimum
* Gradient descent can ﬁnd it ![](.gitbook/assets/104.png)

![](.gitbook/assets/105.png)

Concave function

## Recognizing convex functions

![](.gitbook/assets/106.png)

## Gradient for linear and logistic

## ![](.gitbook/assets/107.png)

Partial derivative w.r.t m

![](.gitbook/assets/108.png)

### Time complexity

![](.gitbook/assets/109.png)

### Codes

![](.gitbook/assets/110.png)

![](.gitbook/assets/111.png)

Example: ![](.gitbook/assets/112.png)

### Learning rate alpha

Learning rate has a signiﬁcant eﬀect on GD

* **too small**: may take a long time to converge
* **too large**: it overshoots

![](.gitbook/assets/113.png)

## Stochastic gradient descent

![](.gitbook/assets/114.png)

Use average (expected value)

| **Batch gradient update**                                                                                                                                                                                              | **Stochastic gradient update**                                                                                                                                                                                                                                     |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| With small learning rate: **guaranteed** improved at each step                                                                                                                                                         | The **steps** are “on average” in the right direction                                                                                                                                                                                                              |
| ![](.gitbook/assets/115.png)                                                                                                                                                                                           | ![](.gitbook/assets/116.png)                                                                                                                                                                                                                                       |
| computes the gradient using the **whole dataset**                                                                                                                                                                      | Stochastic gradient descent (SGD) computes the gradient using a **single sample**. Most applications of SGD actually use a **minibatch** of several samples                                                                                                        |
| Slower                                                                                                                                                                                                                 | Faster                                                                                                                                                                                                                                                             |
| Directly towards an optimum solution, either local or global                                                                                                                                                           | SGD works well for error manifolds that have **lots of local maxima/minima**. In this case, the somewhat noisier gradient calculated using the reduced number of samples tends to jerk the model out of local minima into a region that hopefully is more optimal. |
| ![](.gitbook/assets/117.png)                                                                                                                                                                                           | ![](.gitbook/assets/118.png)                                                                                                                                                                                                                                       |
| [https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent](https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent) |                                                                                                                                                                                                                                                                    |

### Convergence of SGD

Grdients will not reach 0 at optimum, how to guarantee convergence: **schedule** to have a smaller learning rate over time: learning rate is getting smaller while iterating.

![](.gitbook/assets/119.png)

## Minibatch SGD

![](.gitbook/assets/120.png)

### Codes

![](.gitbook/assets/121.png)

![](.gitbook/assets/122.png)

## Momentum

收窄SGD振幅：

* use a running average of gradients
* more recent gradients should have higher weights

![](.gitbook/assets/123.png)

Average moving:

![](.gitbook/assets/124.png)

### Codes

![](.gitbook/assets/125.png)

![](.gitbook/assets/126.png)

## Adagrad (Adaptive gradient)

* use diﬀerent learning rate for each parameter
* make the learning rate adaptive

![](.gitbook/assets/127.png)

useful when parameters are updated at diﬀerent rates (e.g., NLP)

![](.gitbook/assets/128.png)

**problem: the learning rate goes to zero too quickly**

## RMSprop (Root Mean Squared Propagation)

![](.gitbook/assets/129.png)

## Adam (Adaptive Moment Estimation)

two ideas so far:

1\. use momentum to smooth out the oscillations

2\. adaptive per-parameter learning rate

both use exponential moving averages

![](.gitbook/assets/130.png)

![](.gitbook/assets/131.png)

The authors propose default values of 0.9 for β1

, 0.999 for β2, and 10^−8 for ϵ.

## Adding L2 Regularization

![](.gitbook/assets/132.png)

## Adding L1 Regularization

![](.gitbook/assets/133.png)

**Adding regularization can also help with optimization**

Evaluation

## Evaluation and comparison

Each model with same cost functions: report loss

Eg. Least squares or cross entropy

Each model with different cost functions:

Standard evaluation measures/metrics

## Performance metrics for classification

**False positive (type 1)**

**False negative (type 2)**

Eg: patient does not have disease but received positive diagnostic (Type I error)

patient has disease but it was not detected (Type II error) &#x20;

a message that is not spam is assigned to the spam folder (Type I error)

a message that is spam appears in the regular folder (Type II error)

confusion matrix

![](.gitbook/assets/134.png)

![](.gitbook/assets/135.png)

Accuracy, error rate, precision, recall, F1 score:

![](.gitbook/assets/136.png)

Fbeta score

![](.gitbook/assets/137.png)

### Example precision recall

![](.gitbook/assets/138.png)

### Less common metrics:

![](.gitbook/assets/139.png)

## Performance metrics for multi-class classiﬁcation

Report average metrics per class, eg. Average precision

![](.gitbook/assets/140.png)

## Trade-oﬀ between precision and recall ROC\&AUC

![](.gitbook/assets/141.png)

**To compare classiﬁcation algorithms compare their Area Under the Curve (AUC)**

Note that higher AUC doesn’t mean all performance measures are better

![](.gitbook/assets/142.png)

**Also important when comparing ranking algorithms e.g. search results**

**Intuition**: AUC is equivalent to the probability of ranking a random positive example higher than a random negative example

## Cross validation in Evaluation

![](.gitbook/assets/143.png)

Over-ﬁtting in Model Selection

more severe on small dataset and when having too many hyper-parameters but present even with few hyperparameters (小数据更易，很多hyper结果跟很少hyper相似)

### Nested CV

![](.gitbook/assets/144.png)

Perceptron and Support Vector Machines

## Perceptron

### Objective

![](.gitbook/assets/145.png)

![](.gitbook/assets/146.png)

so perceptron tries to minimize the distance of misclassiﬁed points from the decision boundary and push them to the right side

![](.gitbook/assets/147.png)

### Optimization

![](.gitbook/assets/148.png)

### Codes

![](.gitbook/assets/149.png)

Example

![](.gitbook/assets/150.png)

**observations**: after ﬁnding a linear separator no further updates happen; the ﬁnal boundary depends on the order of instances (diﬀerent from all previous methods)

### Issues

![](.gitbook/assets/151.png)

cyclic updates if the data is not linearly separable?

* try make the data separable using additional features?
* data may be inherently noisy

even if linearly separable convergence could take many iterations

the decision boundary may be suboptimal

## Margin

![](.gitbook/assets/152.png)

### Max margin classifier

![](.gitbook/assets/153.png)

![](.gitbook/assets/154.png)

![](.gitbook/assets/155.png)

### Hard margin SVM objective

![](.gitbook/assets/156.png)

![](.gitbook/assets/157.png) ![](.gitbook/assets/158.png)

### Soft margin SVM constraints

allow points inside the margin and on the wrong side but penalize them

![](.gitbook/assets/159.png) ![](.gitbook/assets/160.png)

![](.gitbook/assets/161.png)

![](.gitbook/assets/162.png)

## Hinge loss

Why hinge loss:

We will punish the misclassified data points, which are located either inside the margin or wrong side of the margin, the margin is _distance from boundary._ If classify correctly, then no punishment.

![](.gitbook/assets/163.png)

![](.gitbook/assets/164.png)

![](.gitbook/assets/165.png)

In hard margin SVM there are, by definition, no misclassifications

### Perceptron vs SVM

![](.gitbook/assets/166.png)

* The Perceptron does not try to optimize the separation "distance". As long as it finds a hyperplane that separates the two sets, it is good. SVM on the other hand tries to maximize the "support vector", i.e., the distance between two closest opposite sample points.
* The SVM typically tries to use a "kernel function" to project the sample points to high dimension space to make them linearly separable, while the perceptron assumes the sample points are linearly separable.

The major practical difference between a (kernel) perceptron and SVM is that **perceptrons can be trained online (i.e. their weights can be updated as new examples arrive one at a time)** whereas SVMs cannot be. See this question for information on whether SVMs can be trained online. So, even though a SVM is usually a better classifier, perceptrons can still be useful because they are cheap and easy to re-train in a situation in which fresh training data is constantly arriving.

### ![](.gitbook/assets/167.png)

### SVM codes

![](.gitbook/assets/168.png)

![](.gitbook/assets/169.png)

![](.gitbook/assets/170.png)

![](.gitbook/assets/171.png)

Hard SVM vs soft SVM vs Perceptron

![](.gitbook/assets/172.png)

![](.gitbook/assets/173.png)

## SVM recap

![](.gitbook/assets/174.png)

## SVM vs. logistic regression

![](.gitbook/assets/175.png)

![](.gitbook/assets/176.png)

![](.gitbook/assets/177.png)

Multiclass classification

![](.gitbook/assets/178.png)

![](.gitbook/assets/179.png)

Decision Trees

## Pros and cons

Pros:

decision trees are interpretable!

they are not very sensitive to outliers

do not need data normalization

Cons:

they could easily overﬁt and they are unstable:

avoid by:

* pruning
* random forest

## DT idea

![](.gitbook/assets/180.png)

## Prediction per region

### For regression:

![](.gitbook/assets/181.png)

### For classification:

![](.gitbook/assets/182.png)

## Feature types

### Continuous:

![](.gitbook/assets/183.png)

### Ordinal:

![](.gitbook/assets/184.png)

### Categorical:

![](.gitbook/assets/185.png)

## Cost Function

### Regression cost

![](.gitbook/assets/186.png)

### Classification cost

![](.gitbook/assets/187.png)

## Search space

**Objective**:

ﬁnd a decision tree with K tests minimizing the cost function; alternatively, ﬁnd the **smallest tree (K)** that classiﬁes all examples correctly

Assuming D features, how many diﬀerent partitions of size K+1? the number of full binary trees with K+1 leaves (regions R\_k ) is the Catalan number ![](.gitbook/assets/188.png)

![](.gitbook/assets/189.png) ![](.gitbook/assets/190.png)

## Greedy heuristic

* recursively split the regions based on a greedy choice of the next test
* end the recursion if not worth-splitting

![](.gitbook/assets/191.png)

![](.gitbook/assets/192.png)

![](.gitbook/assets/193.png)

## Stopping the recursion

if we stop when ![](.gitbook/assets/194.png) has zero cost, we may overﬁt heuristics for stopping the splitting:

![](.gitbook/assets/199.png)

## Entropy loss

### Why?

![](.gitbook/assets/200.png)

![](.gitbook/assets/201.png)

![](.gitbook/assets/202.png)

entropy is the expected amount of information in observing a random variable **y**

![](.gitbook/assets/203.png)

## Mutual information

![](.gitbook/assets/204.png) ![](.gitbook/assets/205.png)

## Entropy for classiﬁcation cost

![](.gitbook/assets/206.png)

![](.gitbook/assets/207.png)

## Gini index – the way to split the tree

![](.gitbook/assets/208.png)

While building the decision tree, we would prefer choosing the attribute/feature with the least Gini index as the root node.

Information gain

[https://towardsdatascience.com/gini-index-vs-information-entropy-7a7e4fed3fcb](https://towardsdatascience.com/gini-index-vs-information-entropy-7a7e4fed3fcb)

## Overfitting

![](.gitbook/assets/209.png)

**large decision trees have a high variance – low bias (low training error, high test error)**

1. grow **a small tree**

substantial reduction in cost may happen after a few steps by stopping early we cannot know this

## Pruning

1. grow a large tree and then prune it greedily turn an internal node into a leaf node choice is based on the **lowest increase in the cost** repeat this until left with the root node pick the best among the above models using using a validation set
2. random forests (later!)

Bootstrap, bagging, and boosting

## Bootstrap

![](.gitbook/assets/210.png)

![](.gitbook/assets/211.png)

### Codes

![](.gitbook/assets/212.png)

![](.gitbook/assets/213.png)

![](.gitbook/assets/214.png)

![](.gitbook/assets/215.png)

## Bagging (Bootstrap aggregating)

**Use bootstrap for more accurate prediction (not just uncertainty)**

### Bagging for regression

Will lower variance but same bias

![](.gitbook/assets/216.png)

![](.gitbook/assets/217.png)

### Bagging for classiﬁcation

![](.gitbook/assets/218.png)

mode of iid classiﬁers that are better than chance is a better classiﬁer

* use voting

crowds are wiser when

* individuals are better than random
* votes are uncorrelated

### Example

![](.gitbook/assets/219.png)

## Random forests

Reduce the correlation between decision trees

Feature sub-sampling

only a random subset (![](.gitbook/assets/220.png)) of features are available for split at each step (further reduce the dependence between decision trees)

![](.gitbook/assets/221.png)

### Out of bag (OOB) samples

* the instances not included in a bootstrap dataset can be used for validation
* simultaneous validation of decision trees in a forest
* no need to set aside data for cross validation

![](.gitbook/assets/222.png)

![](.gitbook/assets/223.png)

![](.gitbook/assets/224.png)

**Out Of Bag (OOB)** error can be used for parameter tuning (e.g., size of the forest)

## Summary

* Bootstrap is a powerful technique to get **uncertainty estimates**
* Bootstrap aggregation (Bagging) can reduce the variance of unstable models
* Random forests (**subsample data and features**):
* Bagging + further de-correlation of features at each split
* **OOB validation instead of CV**
* destroy interpretability of decision trees
* perform well in practice
* can fail if only few relevant features exist (due to feature-sampling)

## Adaptive bases

![](.gitbook/assets/225.png)

### Optimization idea

![](.gitbook/assets/226.png)

![](.gitbook/assets/227.png)

### Example

![](.gitbook/assets/228.png)

## Exponential loss

![](.gitbook/assets/229.png)

note that the loss grows faster than the other surrogate losses (more sensitive to outliers)

![](.gitbook/assets/230.png)

AdaBoost

![](.gitbook/assets/231.png)![](.gitbook/assets/232.png)

## AdaBoost algorithm

![](.gitbook/assets/233.png)

![](.gitbook/assets/234.png)

AdaBoost detailed derivations

![](.gitbook/assets/235.png)

Discrete AdaBoost

![](.gitbook/assets/236.png)

![](.gitbook/assets/237.png)

![](.gitbook/assets/238.png)

## Discrete AdaBoost Algorithm Example

![](.gitbook/assets/239.png)![](.gitbook/assets/240.png)

### Example

![](.gitbook/assets/241.png)

**Decision stump: decision tree with one node**

![](.gitbook/assets/242.png)![](.gitbook/assets/243.png)

## Gradient boosting

![](.gitbook/assets/244.png)

![](.gitbook/assets/245.png)

### Algorithm

![](.gitbook/assets/246.png)

## Gradient tree boosting

![](.gitbook/assets/247.png)

![](.gitbook/assets/248.png)

![](.gitbook/assets/249.png)

![](.gitbook/assets/250.png)

two ensemble methods

* bagging & random forests (reduce variance)
  * produce models with minimal correlation
  * use their average prediction
* boosting (reduces the bias of the weak learner)
  * models are added in steps
  * a single cost function is minimized
  * for exponential loss: interpret as re-weighting the instance (AdaBoost)
  * gradient boosting: ﬁt the weak learner to the negative of the gradient
  * interpretation as L1 regularization for "weak learner"-selection
  * also related to max-margin classiﬁcation (for large number of steps T)
* random forests and (gradient) boosting generally perform very well

## Some loss functions for gradient boosting

![](.gitbook/assets/251.png)

## !!!Boosting vs Bagging

1、Bagging (bootstrap aggregating)

Bagging即套袋法，其算法过程如下：

A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstrapping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，with replacement而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）

B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）

C）对分类问题：将上步得到的k个模型采用投票voting的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

2、Boosting

其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。

关于Boosting的两个核心问题：

1）在每一轮如何改变训练数据的权值或概率分布？

通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。

2）通过什么方式来组合弱分类器？

通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。

而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

3、Bagging，Boosting二者之间的区别

Bagging和Boosting的区别：

1）样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

2）样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。(分错多的多权重)

3）预测函数：

Bagging：所有预测函数的权重相等。

Boosting：每个弱分类器都有相应的权重，最后voting时， 对于分类误差小的分类器会有更大的权重。

4）并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

[https://www.cnblogs.com/liuwu265/p/4690486.html](https://www.cnblogs.com/liuwu265/p/4690486.html)

Maximum likelihood estimation:

1. given a pdf
2. ![](.gitbook/assets/252.png)the pdf w.r.t X
3. Log step 2
4. Get the derivate = 0 w.r.t the estimator you would like to
5. Get the MLE w.r.t the estimator

[https://daviddalpiaz.github.io/stat3202-sp19/homework/pp-03-soln.pdf](https://daviddalpiaz.github.io/stat3202-sp19/homework/pp-03-soln.pdf)

pg. 4

![](.gitbook/assets/253.png)

Multilayer perceptron

![](.gitbook/assets/254.png)

## Adaptive Radial Base

![](.gitbook/assets/255.png)

![](.gitbook/assets/256.png)

![](.gitbook/assets/257.png)

## Sigmoid Bases

![](.gitbook/assets/258.png)

![](.gitbook/assets/259.png)

### Adaptive sigmoid bases

![](.gitbook/assets/260.png)

## Multilayer perceptron MLP

![](.gitbook/assets/261.png)

![](.gitbook/assets/262.png)

## Regression

![](.gitbook/assets/263.png)

![](.gitbook/assets/264.png)

## Classification

![](.gitbook/assets/265.png)

![](.gitbook/assets/266.png)

## !!! Activation function

### Logistic function

### Hyperbolic tangent

![](.gitbook/assets/267.png)

### ReLU

### Leaky ReLU

### Softplus

![](.gitbook/assets/268.png)

## Network architecture

Feedforward network aka multilayer perceptron

![](.gitbook/assets/269.png)

![](.gitbook/assets/270.png)

## Depth vs Width

![](.gitbook/assets/271.png)

![](.gitbook/assets/272.png)

![](.gitbook/assets/273.png)

## Multilayer perceptron

![](.gitbook/assets/274.png)

![](.gitbook/assets/275.png)

![](.gitbook/assets/276.png)

![](.gitbook/assets/277.png)

![](.gitbook/assets/278.png)

![](.gitbook/assets/279.png)

![](.gitbook/assets/280.png)

fully-connected: all outputs of one layer's units are input to all the next units

![](.gitbook/assets/281.png)

## Regularization strategies

### Overfit: variance reduction

![](.gitbook/assets/282.png)

Data augmentation (增大)

![](.gitbook/assets/283.png)

![](.gitbook/assets/284.png)

## Noise robustness

![](.gitbook/assets/285.png)

## Early stopping

![](.gitbook/assets/286.png)

## Bagging

![](.gitbook/assets/287.png)

## Dropout

![](.gitbook/assets/288.png)

![](.gitbook/assets/289.png)

Let's start with **normal dropout**, i.e. dropout only at training time. Here dropout serves as a regularization to **avoid overfitting**. During test time, dropout is not applied; instead, all nodes/connections are present, but the weights are adjusted accordingly(e.g. multiply the dropout ratio). Such a model during test time can be understood as a average of an ensemble of neural networks.

Notice that for normal dropout, at test time the prediction is **deterministic**. _Without other source of randomness, given one test data point, the model will always predict the same label or value_.

For **Monte Carlo dropout**, the dropout is applied at both training and test time. At test time, the prediction is **no longer** **deterministic**, but depending on which nodes/links you randomly choose to keep. Therefore, _given a same datapoint, your model could predict different values each time._

So the primary goal of Monte Carlo dropout is to generate random predictions and **interpret them as samples from a probabilistic distribution**. In the authors' words, they call it Bayesian interpretation.

Example: suppose you trained an dog/cat image classifier with Monte Carlo dropout. If you feed a same image to the classifier again and again, the classifier may be predicting dog 70% of the times while predicting cat 30% of the time. Therefore you can interpret the result in a probabilistic way: with 70% probability, this image shows a dog.

## Summary

* Deep feed-forward networks learn adaptive bases
* more complex bases at higher layers
* increasing depth is often preferable to width
* various choices of activation function and architecture
* universal approximation power
* their expressive power often necessitates using regularization schemes

&#x20;Gradient Computation & Automatic Differentiation

## Landscape of the cost function

![](.gitbook/assets/290.png)

![](.gitbook/assets/291.png)

![](.gitbook/assets/292.png)

## Jacobian matrix

![](.gitbook/assets/293.png)

## Chain rule

![](.gitbook/assets/294.png)

## Training a two layer MLP

![](.gitbook/assets/295.png)

## Gradient calculation

![](.gitbook/assets/296.png)

### For regression

![](.gitbook/assets/297.png)

### For binary classification

![](.gitbook/assets/298.png)

### For multiclass classification

![](.gitbook/assets/299.png)

Softmax: ![](.gitbook/assets/300.png)

Code:

def softmax( u, # N x K ): u\_exp = np.exp(u - np.max(u, 1)\[:, None])

return u\_exp / np.sum(u\_exp, axis=-1)\[:, None]

xs = np.array(\[-1, 0, 3, 5])

print(softmax(xs)) # \[0.0021657, 0.00588697, 0.11824302, 0.87370431]

Cross entropy: ![](.gitbook/assets/301.png)

def cross\_entropy(p, q):

&#x20;return -sum(\[p\[i]\*log2(q\[i]) for i in range(len(p))])

![](.gitbook/assets/302.png)

### Example:

![](.gitbook/assets/303.png)

Codes:

![](.gitbook/assets/304.png)

![](.gitbook/assets/305.png)

![](.gitbook/assets/306.png)

![](.gitbook/assets/307.png)

![](.gitbook/assets/308.png)

![](.gitbook/assets/309.png)

## Automating gradient computation

![](.gitbook/assets/310.png)

## Automatic differentiation

![](.gitbook/assets/311.png)

Backpropagation

## Forward mode

![](.gitbook/assets/312.png)

### Computational graph

![](.gitbook/assets/313.png)

## Reverse mode

![](.gitbook/assets/314.png)

### Computational graph

![](.gitbook/assets/315.png)

## Forward vs Reverse mode

![](.gitbook/assets/316.png)

## Summary

![](.gitbook/assets/317.png)

Backpropagation explain and resources

Suppose two hidden layers MLP:

![](.gitbook/assets/318.png)

**Backpropagation aims to find which weight/bias/activation function has the relatively larger influence on minimize the cost and update them correspondingly.**

For example:

![](.gitbook/assets/319.png)

This is the cost – weight/bias of hidden layer 2, to calculate the ratio between the weights (and biases) and the cost function. The ones with the largest ratio will have the greatest impact on the cost function and will give us 'the most bang for our buck'.

Ex. We want to increase the prob to classify it into 2, we would like to know which neuron’s weight/bias/activation has larger influence so that we can adjust them to get our desired output - 2, such as the yellow line.

![](.gitbook/assets/320.png)

![](.gitbook/assets/321.png)![](.gitbook/assets/322.png)![](.gitbook/assets/323.png)

We will have to average the changes

![](.gitbook/assets/324.png)![](.gitbook/assets/325.png)

Math formula (output – the last hidden layer):

![](.gitbook/assets/326.png)

![](.gitbook/assets/327.png)

![](.gitbook/assets/328.png)

With multiple layers and neurons:

![](.gitbook/assets/329.png)

Calculating the gradient

![](.gitbook/assets/330.png)

Formula for the hidden layer 1:

![](.gitbook/assets/331.png)

Extra hidden layer:

![](.gitbook/assets/332.png)

**Summarization**

![](.gitbook/assets/333.png)

Resources:

[https://mlfromscratch.com/neural-networks-explained/#backpropagation](https://mlfromscratch.com/neural-networks-explained/#backpropagation)

[https://www.youtube.com/watch?v=Ilg3gGewQ5U](https://www.youtube.com/watch?v=Ilg3gGewQ5U)
